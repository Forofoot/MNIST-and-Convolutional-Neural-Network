{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9f58194-4b2c-434f-a18a-c48205179611",
   "metadata": {},
   "source": [
    "# Cellule 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8097ee8-10a3-4159-92cc-75b07e31a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d5a74b-8006-4339-b77e-5ecfe5c07485",
   "metadata": {},
   "source": [
    "# Cellule 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e346d46-338b-46f2-83f7-2c8cfa4424bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c683c36",
   "metadata": {},
   "source": [
    "# Ne pas éxecuter pour le moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65a74e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/raw\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/raw\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../data/raw\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/raw\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/raw\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data/raw\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/raw\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/raw\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data/raw\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/raw\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/raw\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data/raw\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tf = transforms.Compose([transforms.ToTensor(),\n",
    "transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST(\"../data/raw\", download=True, train=True, transform=tf),\n",
    "batch_size=64, shuffle=True)\n",
    "test_load = torch.utils.data.DataLoader(datasets.MNIST(\"../data/raw\", download=True, train=False, transform=tf),\n",
    "batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f15932",
   "metadata": {},
   "source": [
    "## Début du code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76755d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Récupérer les 5 premières images et leurs étiquettes\n",
    "batch = next(iter(train_loader))\n",
    "images = batch[0][:5]\n",
    "labels = batch[1][:5]\n",
    "\n",
    "# Afficher les images\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(images[i].squeeze(), cmap='gray')\n",
    "    ax.set_title(f\"Label: {labels[i]}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1327ca52",
   "metadata": {},
   "source": [
    "### Construction d'une classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e153717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_size, n_kernels, output_size):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, n_kernels, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(n_kernels, n_kernels, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_kernels * 4 * 4, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, output_size)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a545452e",
   "metadata": {},
   "source": [
    "### Fonction train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "299189f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, perm, n_epoch=1):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Appliquer les permutations de pixels par la matrice circulaire de Toeplitz\n",
    "            data = data.view(-1, 28*28)\n",
    "            data = data[:, perm]\n",
    "            data = data.view(-1, 1, 28, 28)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(data)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch: {epoch}, Step: {batch_idx}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574ab473",
   "metadata": {},
   "source": [
    "### Fonction test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7894645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, perm):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Appliquer les permutations de pixels par la matrice circulaire de Toeplitz\n",
    "            data = data.view(-1, 28*28)\n",
    "            data = data[:, perm]\n",
    "            data = data.view(-1, 1, 28, 28)\n",
    "            \n",
    "            logits = model(data)\n",
    "            test_loss += F.cross_entropy(logits, target, reduction='sum').item()\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    print(f'Average loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8412a420",
   "metadata": {},
   "source": [
    "### Lancement de l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c6bf1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters=6.422K\n",
      "Epoch: 0, Step: 0, Loss: 2.295722484588623\n",
      "Epoch: 0, Step: 100, Loss: 0.5455559492111206\n",
      "Epoch: 0, Step: 200, Loss: 0.4954688251018524\n",
      "Epoch: 0, Step: 300, Loss: 0.21575330197811127\n",
      "Epoch: 0, Step: 400, Loss: 0.2976612150669098\n",
      "Epoch: 0, Step: 500, Loss: 0.2254517674446106\n",
      "Epoch: 0, Step: 600, Loss: 0.17761041224002838\n",
      "Epoch: 0, Step: 700, Loss: 0.05773052200675011\n",
      "Epoch: 0, Step: 800, Loss: 0.17278288304805756\n",
      "Epoch: 0, Step: 900, Loss: 0.03685908764600754\n",
      "Average loss: 0.1168, Accuracy: 0.9618\n"
     ]
    }
   ],
   "source": [
    "n_kernels = 6\n",
    "input_size = 28*28\n",
    "output_size = 10\n",
    "convnet = ConvNet(input_size, n_kernels, output_size)\n",
    "convnet.to(device)\n",
    "print(f\"Parameters={sum(p.numel() for p in convnet.parameters())/1e3}K\")\n",
    "train(convnet, train_loader, torch.arange(0, 784))\n",
    "test(convnet, test_load, torch.arange(0, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc200e8b",
   "metadata": {},
   "source": [
    "### Nouveau modèle : le perceptron multi-couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "100d0314",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ab6769",
   "metadata": {},
   "source": [
    "### Fonction train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47a88a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_train(model, train_loader, perm, n_epoch=1):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Appliquer les permutations de pixels par la matrice circulaire de Toeplitz\n",
    "            data = data.view(-1, 28*28)\n",
    "            data = data[:, perm]\n",
    "            data = data.view(-1, 1, 28, 28)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(data)\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch: {epoch}, Step: {batch_idx}, Loss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07eb30c",
   "metadata": {},
   "source": [
    "### Fonction test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f218afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_test(model, test_loader, perm):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Appliquer les permutations de pixels par la matrice circulaire de Toeplitz\n",
    "            data = data.view(-1, 28*28)\n",
    "            data = data[:, perm]\n",
    "            data = data.view(-1, 1, 28, 28)\n",
    "            \n",
    "            logits = model(data)\n",
    "            test_loss += F.cross_entropy(logits, target, reduction='sum').item()\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    print(f'Average loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663ceac",
   "metadata": {},
   "source": [
    "### Lancement de l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "774483c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters=6.442K\n",
      "Epoch: 0, Step: 0, Loss: 2.328458786010742\n",
      "Epoch: 0, Step: 100, Loss: 1.3120949268341064\n",
      "Epoch: 0, Step: 200, Loss: 0.9425003528594971\n",
      "Epoch: 0, Step: 300, Loss: 0.5962355136871338\n",
      "Epoch: 0, Step: 400, Loss: 0.5497626662254333\n",
      "Epoch: 0, Step: 500, Loss: 0.5512579679489136\n",
      "Epoch: 0, Step: 600, Loss: 0.36224350333213806\n",
      "Epoch: 0, Step: 700, Loss: 0.8227468729019165\n",
      "Epoch: 0, Step: 800, Loss: 0.3002803921699524\n",
      "Epoch: 0, Step: 900, Loss: 0.5813274383544922\n",
      "Average loss: 0.4043, Accuracy: 0.8804\n"
     ]
    }
   ],
   "source": [
    "input_size = 28*28\n",
    "output_size = 10\n",
    "n_hidden = 8\n",
    "mlp = MLP(input_size, n_hidden, output_size)\n",
    "mlp.to(device)\n",
    "print(f\"Parameters={sum(p.numel() for p in mlp.parameters())/1e3}K\")\n",
    "mlp_train(mlp, train_loader, torch.arange(0, 784))\n",
    "mlp_test(mlp, test_load, torch.arange(0, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb11414",
   "metadata": {},
   "source": [
    "Bien que le MLP soit plus simple et flexible, il n'est pas aussi bien adapté pour traiter des données structurées en grille comme les images. Les CNNs, en revanche, sont spécifiquement conçus pour exploiter les structures spatiales des images, ce qui les rend beaucoup plus efficaces pour les tâches de vision par ordinateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36955ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters=6.442K\n",
      "Epoch: 0, Step: 0, Loss: 2.621769428253174\n",
      "Epoch: 0, Step: 100, Loss: 0.7604302763938904\n",
      "Epoch: 0, Step: 200, Loss: 0.5023573637008667\n",
      "Epoch: 0, Step: 300, Loss: 0.4332887828350067\n",
      "Epoch: 0, Step: 400, Loss: 0.29884347319602966\n",
      "Epoch: 0, Step: 500, Loss: 0.35487493872642517\n",
      "Epoch: 0, Step: 600, Loss: 0.3657197058200836\n",
      "Epoch: 0, Step: 700, Loss: 0.19481995701789856\n",
      "Epoch: 0, Step: 800, Loss: 0.5254439115524292\n",
      "Epoch: 0, Step: 900, Loss: 0.31269392371177673\n",
      "Average loss: 3.3356, Accuracy: 0.0693\n",
      "-----------------\n",
      "Parameters=6.422K\n",
      "Epoch: 0, Step: 0, Loss: 4.334047317504883\n",
      "Epoch: 0, Step: 100, Loss: 1.4392523765563965\n",
      "Epoch: 0, Step: 200, Loss: 0.6984210014343262\n",
      "Epoch: 0, Step: 300, Loss: 0.8331004977226257\n",
      "Epoch: 0, Step: 400, Loss: 0.4615146219730377\n",
      "Epoch: 0, Step: 500, Loss: 0.6334160566329956\n",
      "Epoch: 0, Step: 600, Loss: 0.6140337586402893\n",
      "Epoch: 0, Step: 700, Loss: 0.41066673398017883\n",
      "Epoch: 0, Step: 800, Loss: 0.5583491921424866\n",
      "Epoch: 0, Step: 900, Loss: 0.6091879606246948\n",
      "Average loss: 3.6665, Accuracy: 0.1375\n"
     ]
    }
   ],
   "source": [
    "print(f\"Parameters={sum(p.numel() for p in mlp.parameters())/1e3}K\")\n",
    "mlp_train(mlp, train_loader, torch.randperm(784))\n",
    "mlp_test(mlp, test_load, torch.randperm(784))\n",
    "print(\"-----------------\")\n",
    "print(f\"Parameters={sum(p.numel() for p in convnet.parameters())/1e3}K\")\n",
    "train(convnet, train_loader, torch.randperm(784))\n",
    "test(convnet, test_load, torch.randperm(784))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69012a50",
   "metadata": {},
   "source": [
    "Cette expérience démontre clairement l'importance de la structure spatiale des images pour les CNNs. Les CNNs sont conçus pour tirer parti des relations locales entre les pixels, ce qui leur donne un avantage significatif sur les MLPs pour les tâches de vision par ordinateur. Cependant, lorsque cette structure est détruite par une permutation aléatoire, les CNNs perdent cet avantage et leurs performances chutent considérablement.\n",
    "\n",
    "Les MLPs, bien que moins dépendants de la structure spatiale, ne sont pas capables de compenser cette perte car ils n'exploitent pas les relations spatiales de manière efficace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46590be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(convnet.state_dict(), \"convnet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e207e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
